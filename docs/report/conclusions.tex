\chapter{Conclusions} \label{chap:conclusions}
The 'sigle implementation' of CUDA Keccak did not achieve effective performance improvements if compared to the CPU reference version of the algorithm.\\
This result was not surprising because of several reasons:
\begin{description}
\item [Intrinsic Sequentiality of Keccak] Due to the fact that each step of the algorithm needs the results of the previous one before starting, only the operations belonging to the current step can be actually executed in parallel. This situation leads to a low exploitation of the GPU resources. As described in Section \ref{chap:design}, only 25 threads are used, and furthermore this number does not scale with the capabilities of the GPU device.
\item [Arithmetic Operations] Some operations required by the Keccak algorithm, like SHIFT-64 or bit to bit XOR, reduce the performance in terms of instructions per seconds. Trying to avoid the use of those kind of operations in the algorithm implementation is equivalent to rewriting the algorithm itself. Resuming, a few threads performing rather slow operations leads to an under-exploitation of the possibilities offered by the Cuda Framework.
\end{description}
The 'multi implementation' instead has actually obtained a significant speed-up \dots
As expected, devices with a compute cap lower than 1.3 \dots
However not enough, expecially supposing a multithreading cpu implementation \dots
The reason behind this are many, but one is probably the most important \dots
Local memory is a memory abstraction that implies "local in the scope of each thread". It is not an actual hardware component of the multi-processor. In actuality, local memory resides in global memory allocated by the compiler and delivers the same performance as any other global memory region. Normally, automatic variables declared in a kernel reside in registers, which provide very fast access. Unfortunately, the relationship between automatic variables and local memory continues to be a source of confusion for CUDA programmers. The compiler might choose to place automatic variables in local memory when:
\begin{itemize}
\item There are too many register variables.
\item The compiler cannot determine if an array is indexed with constant quantities. Please note that registers are not addressable so an array has to go into local memory -- even if it is a two-element array -- when the addressing of the array is not known at compile time.
\item \textbf{A structure would consume too much register space}.
\end{itemize}
In this work, expecially the last one of the previous has been a big problem \dots
\section{Future Developments Suggestions}
There are several ways in which this work can be extended and improved, event though these considerations are out of the scope:
\begin{description}
\item [CuKeccak] Design a brand new algorithm, well suited for parallelism, implementing the same hash-function.
\item [OpenCL] \dots
\end{description}
